version: '3.7'

services:
  web:
    restart: always
    image: web
    build:
      context: ./app/kafka_workers/controller
      dockerfile: Dockerfile
    command: watchmedo auto-restart -d . -p '*.py' --recursive -- gunicorn -w ${WORKERS:-4} -b 0.0.0.0:5000 main:app --timeout=1200  --log-level=${LOG_LEVEL:-debug}
    ports:
      - 5001:5000
    volumes:
      - ./app/kafka_workers/controller:/usr/src/app/
    environment:
      WORKERS: 1
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_USERNAME: ${MONGO_INITDB_USERNAME}
      MONGO_INITDB_PASSWORD: ${MONGO_INITDB_PASSWORD}
      INITDB_COLLECTION: ${INITDB_COLLECTION}
    networks:
      local-network:
        aliases:
          - controller
    depends_on: 
      - mongo
      - es01

  watcher_producer:
    restart: always
    image: argus/watcher
    container_name: watcher
    build:
      context: .
      dockerfile: Dockerfile
    command: watchmedo auto-restart -d app/kafka_workers/watcher -p '*.py' --recursive -- python app/kafka_workers/watcher/watcher_producer.py
    volumes:
      - .:/usr/src/app/
    environment:
      WORKERS: 1
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_USERNAME: ${MONGO_INITDB_USERNAME}
      MONGO_INITDB_PASSWORD: ${MONGO_INITDB_PASSWORD}
      INITDB_COLLECTION: ${INITDB_COLLECTION}
    networks:
      local-network:
        aliases:
          - analysis_consumer
    depends_on: 
      - scraper_consumer
      - kafka1

  scraper_consumer:
    restart: always
    image: argus/scraper
    container_name: scraper
    build:
      context: .
      dockerfile: Dockerfile
    command: watchmedo auto-restart -d app/kafka_workers/scraper -p '*.py' --recursive -- python app/kafka_workers/scraper/scraper_consumer.py
    volumes:
      - .:/usr/src/app/
    environment:
      WORKERS: 1
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_USERNAME: ${MONGO_INITDB_USERNAME}
      MONGO_INITDB_PASSWORD: ${MONGO_INITDB_PASSWORD}
      INITDB_COLLECTION: ${INITDB_COLLECTION}
    networks:
      local-network:
        aliases:
          - scraper_consumer
    depends_on: 
      - content_miner
      - kafka1

  content_miner:
    restart: always
    image: argus/content_miner
    container_name: content_miner
    build:
      context: .
      dockerfile: Dockerfile
    command: watchmedo auto-restart -d app/kafka_workers/content_miner -p '*.py' --recursive -- python app/kafka_workers/content_miner/content_miner_consumer.py
    volumes:
      - .:/usr/src/app/
    environment:
      TIKA_CLIENT_ONLY: "True"
      TIKA_VERSION: "1.24"
      TIKA_SERVER_ENDPOINT: "http://tika-server:9998"
      TIKA_JAVA_ARGS: "-Xmx2g -Xmx2g"
      PYTHONIOENCODING: utf8
      WORKERS: 1
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_USERNAME: ${MONGO_INITDB_USERNAME}
      MONGO_INITDB_PASSWORD: ${MONGO_INITDB_PASSWORD}
      INITDB_COLLECTION: ${INITDB_COLLECTION}
      TRANSLATOR_TEXT_SUBSCRIPTION_KEY: ${TRANSLATOR_TEXT_SUBSCRIPTION_KEY}
      TRANSLATOR_TEXT_ENDPOINT: ${TRANSLATOR_TEXT_ENDPOINT}
    networks:
      local-network:
        aliases:
          - content_miner
    depends_on:
      - analysis_consumer
      - tika
      - kafka1

  analysis_consumer:
    restart: always
    image: argus/analyser
    container_name: analyser
    build:
      context: .
      dockerfile: Dockerfile
    command: watchmedo auto-restart -d app/kafka_workers/analyser -p '*.py' --recursive -- python app/kafka_workers/analyser/analyser_consumer.py
    volumes:
      - .:/usr/src/app/
    environment:
      WORKERS: 1
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_USERNAME: ${MONGO_INITDB_USERNAME}
      MONGO_INITDB_PASSWORD: ${MONGO_INITDB_PASSWORD}
      INITDB_COLLECTION: ${INITDB_COLLECTION}
    networks:
      local-network:
        aliases:
          - analysis_consumer
    depends_on: 
      - kafka1

  tika:
    image: lexpredict/tika-server:1.13
    restart: always
    ports:
      - 9998:9998
    networks:
      local-network:
        aliases: 
          - tika-server

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.1
    container_name: es01
    ports:
      9200:9200
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es01
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      local-network:
        aliases: 
          - elasticsearch

  mongo:
    image: mongo:latest
    container_name: "mongodb"
    environment:
      MONGO_DATA_DIR: /data/db
      MONGO_LOG_DIR: /dev/null
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_INITDB_DATABASE}
      MONGO_INITDB_USERNAME: ${MONGO_INITDB_USERNAME}
      MONGO_INITDB_PASSWORD: ${MONGO_INITDB_PASSWORD}
      INITDB_COLLECTION: ${INITDB_COLLECTION}
    volumes:
      - ./extras/mongo/init-mongo.sh:/docker-entrypoint-initdb.d/init-mongo.sh
      - ./extras/mongo/data/db:/data/db
    ports:
      - 27017:27017
    networks:
      local-network:
        aliases:
          - mongodb

  zookeeper:
    image: confluentinc/cp-zookeeper:5.2.1
    restart: always
    container_name: confluentinc_zookeeper
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: "2181"
      ZOOKEEPER_TICK_TIME: "2000"
      ZOOKEEPER_SERVERS: "zookeeper:22888:23888"
    ports:
      - "2181:2181"
    volumes:
      - ./extras/zookeeper/data:/data
      - ./extras/zookeeper/datalog:/datalog
    networks:
      - local-network
            
  connect:
    image: confluentinc/cp-kafka-connect:5.2.1
    container_name: confluentinc_connect
    depends_on:
      - zookeeper
      - kafka1
      - schemaregistry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka1:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schemaregistry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schemaregistry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java'    
    networks:
      - local-network
      
  kafka1:
    image: confluentinc/cp-enterprise-kafka:5.2.1
    container_name: confluentinc_kafka1
    depends_on:
      - zookeeper
    ports:
    # Exposes 29092 for external connections to the broker
    # Use kafka1:9092 for connections internal on the docker network
    # See https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
      - "29092:29092"
      - "9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: "r1"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_SCHEMA_REGISTRY_URL: "schemaregistry:8081"
      #KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_JMX_PORT: 9991
    volumes:
      - ./extras/kafka/data:/var/lib/kafka/data
    networks:
      - local-network

  schemaregistry:
    image: confluentinc/cp-schema-registry:5.2.1
    container_name: confluentinc_schemaregistry
    restart: always
    depends_on:
      - zookeeper
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "zookeeper:2181"
      SCHEMA_REGISTRY_HOST_NAME: schemaregistry
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
    ports:
      - 8081:8081
    networks:
      - local-network
      
  restproxy:
    image: confluentinc/cp-kafka-rest:5.2.1
    container_name: confluentinc_restproxy
    restart: always
    depends_on:
      - kafka1
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: "http://schemaregistry:8081"
      KAFKA_REST_HOST_NAME: restproxy
      KAFKA_REST_DEBUG: "true"
    ports:
      - 8082:8082
    networks:
      - local-network

volumes:
  data01:
    driver: local

networks:
  local-network:
    driver: bridge
